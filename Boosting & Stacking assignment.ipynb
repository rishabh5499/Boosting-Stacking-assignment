{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edd3fa2a-6bfb-4631-bc7e-ffa389786b2c",
   "metadata": {},
   "source": [
    "1. What is Boosting in Machine Learning? Explain how it improves weak learners.\n",
    "   - Boosting is an ensemble learning technique in machine learning that combines multiple weak learners (models that perform slightly better than random guessing) to form a strong predictive model.\n",
    "   - Boosting works by training models sequentially, where each new model focuses more on the mistakes made by previous models. Samples that were misclassified earlier are given higher importance (weights), forcing subsequent learners to learn difficult patterns.\n",
    "   - How it improves weak learners:\n",
    "   - Combines many simple models into a strong one\n",
    "   - Reduces both bias and variance\n",
    "   - Focuses learning on hard-to-predict data points\n",
    "   - Improves overall accuracy and robustness\n",
    "   - Popular boosting algorithms include AdaBoost, Gradient Boosting, XGBoost, and CatBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d1d16d-0ca2-4f0b-b029-63af196ebe1a",
   "metadata": {},
   "source": [
    "2.  What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?\n",
    "    - AdaBoost re weights misclassified samples and gradient booting fits the model to residual errors.\n",
    "    - AdaBoost uses sample weighting and gradient boosting performs error minimization.\n",
    "    - AdaBoost is highly sensitive to noise and Gradient boosting is less sensitive to noise.\n",
    "    - AdaBoost decission stumps as a common base learner and Gradient boosting uses decission trees.\n",
    "    - AdaBoost uses implicit loss optimization technique and Gradient boosting uses explicit gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604a60d8-7096-4b08-864e-210354dcbf0e",
   "metadata": {},
   "source": [
    "3. How does regularization help in XGBoost?\n",
    "   - Regularization in XGBoost helps prevent overfitting by penalizing model complexity.\n",
    "   - XGBoost introduces:\n",
    "   - L1 regularization (alpha) – penalizes absolute leaf weights\n",
    "   - L2 regularization (lambda) – penalizes squared leaf weights\n",
    "   - Tree structure penalties – limits tree depth and number of leaves\n",
    "\n",
    "   - Benefits:\n",
    "   - Controls model complexity\n",
    "   - Improves generalization on unseen data\n",
    "   - Reduces variance\n",
    "   - Produces more stable predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a6957a-43eb-4d29-ad7c-9df77e507e89",
   "metadata": {},
   "source": [
    "4. Why is CatBoost considered efficient for handling categorical data?\n",
    "   - CatBoost is designed to handle categorical features natively without manual encoding.\n",
    "\n",
    "   - Reasons for efficiency:\n",
    "   - Uses ordered target encoding (prevents data leakage)\n",
    "   - Automatically processes categorical variables\n",
    "   - Eliminates need for one-hot encoding\n",
    "   - Reduces overfitting using permutation-driven encoding\n",
    "   - Performs well on small and medium datasets\n",
    "   - This makes CatBoost especially suitable for real-world datasets with many categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e91f33-6a96-40ed-a39c-223f2fea1646",
   "metadata": {},
   "source": [
    "5. What are some real-world applications where boosting techniques are preferred over bagging methods?\n",
    "   - Boosting is preferred when accuracy and learning complex patterns matter more than speed.\n",
    "\n",
    "   - Applications:\n",
    "   - Credit risk and loan default prediction\n",
    "   - Fraud detection\n",
    "   - Medical diagnosis (cancer detection)\n",
    "   - Customer churn prediction\n",
    "   - Search ranking systems\n",
    "   - Recommendation engines\n",
    "\n",
    "   - Boosting handles imbalanced datasets and complex relationships better than bagging methods like Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64acf1c7-4503-4782-bde4-9341664a3f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "# 6. Write a Python program to:\n",
    "# ● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
    "# ● Print the model accuracy\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "218936a1-f963-42b0-aad7-b330896c5e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared Score: 0.7756446042829697\n"
     ]
    }
   ],
   "source": [
    "# 7. Write a Python program to:\n",
    "# ● Train a Gradient Boosting Regressor on the California Housing dataset\n",
    "# ● Evaluate performance using R-squared score\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model = GradientBoostingRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"R-squared Score:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0de18126-4570-4e67-8c40-08418c39b712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.2}\n",
      "Accuracy: 0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "grid = GridSearchCV(model, param_grid, cv=3)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11042a19-cb61-49c3-ab55-55dae5c51477",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'catboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 9. Write a Python program to:\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# ● Train a CatBoost Classifier\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# ● Plot the confusion matrix using seaborn\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcatboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CatBoostClassifier\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_breast_cancer\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'catboost'"
     ]
    }
   ],
   "source": [
    "# 9. Write a Python program to:\n",
    "# ● Train a CatBoost Classifier\n",
    "# ● Plot the confusion matrix using seaborn\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model = CatBoostClassifier(verbose=0, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a679a2-d9a2-4ab6-8783-b668ca702354",
   "metadata": {},
   "source": [
    "10. You're working for a FinTech company trying to predict loan default using customer demographics and transaction behavior.\n",
    "The dataset is imbalanced, contains missing values, and has both numeric and categorical features.\n",
    "Describe your step-by-step data science pipeline using boosting techniques:\n",
    "● Data preprocessing & handling missing/categorical values\n",
    "● Choice between AdaBoost, XGBoost, or CatBoost\n",
    "● Hyperparameter tuning strategy\n",
    "● Evaluation metrics you'd choose and why\n",
    "● How the business would benefit from your model\n",
    "\n",
    "\n",
    "- Data Preprocessing\n",
    "- Handle missing values:\n",
    "- Numeric → median imputation\n",
    "- Categorical → CatBoost handles internally\n",
    "- Remove outliers\n",
    "- Train-test split with stratification\n",
    "\n",
    "- Algorithm Choice\n",
    "- CatBoost\n",
    "- Handles categorical data natively\n",
    "- Robust to missing values\n",
    "- Prevents target leakage\n",
    "- Performs well on imbalanced datasets\n",
    "\n",
    "\n",
    "- Hyperparameter Tuning\n",
    "- Use GridSearchCV / RandomizedSearchCV\n",
    "- Tune:\n",
    "- learning_rate\n",
    "- depth\n",
    "- iterations\n",
    "- class_weights\n",
    "\n",
    "\n",
    "- Evaluation Metrics\n",
    "- ROC-AUC → overall discrimination\n",
    "- Precision & Recall → reduce false negatives\n",
    "- F1-score → balance precision and recall\n",
    "\n",
    "\n",
    "- Business Benefits\n",
    "- Reduced loan default risk\n",
    "- Better credit decisioning\n",
    "- Increased profitability\n",
    "- Improved regulatory compliance\n",
    "- Data-driven lending strategy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
